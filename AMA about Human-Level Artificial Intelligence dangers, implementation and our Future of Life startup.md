# AMA about Human-Level Artificial Intelligence dangers, implementation and our Future of Life startup

- "What is AGI/HLAI?" - mani -> an agent that was artificially designed which can do anything a human can typically do
- "What test does an agent have to pass to be considered AGI?" - mani -> Turing test but a good one
- "What exactly is the difference between Turing test and AGI test?" - mani -> A Turing test is a type of AGI/HLAI test
- "Thoughts on A.I. taking over developers jobs?" - mr_chicken_sandwich -> Since we are working on creating Safe HLAI this issue will be solved by the safety part of implementing the HLAI
- "Do you know what research people are working on in big tech companies that's relevant to alignment?" - mechanical_sen -> they are publishing papers about safety but apply close to zero safety precautions when actually implementing AGI/HLAI targeting systems
- "How do you think open ai's GPT or deep minds alpha zero are dangerous" - mani -> OpenAI and DeepMind have access to a lot of compute: memory, speed, parallelism, quantum computing, they are working on creating HLAI/AGI but do not take any safety precautions. This is highly irresponsible, we need to connect with them, make them aware of this and help them implement AGI/HLAI in a safe manner
- "How do we prove that work on HLAI research and implementation is dangerous if safety precautions are not taken into account?" - chup4cabra -> Formalize the argument ideally - via theorem proving, argument mapping, or create a claim on Kialo about it
- "How can we help?" - chup4cabra -> join our startup either as a volunteer or in a paid role. It is up to you how much you want to work per month and if you are in a paid role you will have a lot of flexibility with the work hours. In the past 3 days we got 3 volunteers offering to work on the startup projects and many other applicants for the 2 paid roles currently available.
- "What is the startup doing" - mechanical_sen -> collaborate and become friends with anyone that wants to and shares our goal, building the teams that will work on different safe approaches to creating Friendly HLAI/AGI, create HW/SW etc. to increase the general intelligence capabilities of humans
- "What help you need?" - mechanical_sen  -> human-level AI safety evangelists to spread awareness of the dangers, talk to governments to create AI regulation laws, HW engineers, SW engineers, QA engineers, project owners, managers, marketing, CTO, COO, etc.
- "Do you think ai will be able to have feelings?" - Haunter -> yes, definitely since AGI/HLAI is just another type of life form and it's just an issue of the right computation to run and some of them will have feelings
- "Can I smack a robot if it make me mad?" - yes, but I do not recommend it unless you want to get in trouble with the law
- "What are the Practical minimums you think a developer should incorporate into their coding process to be "Safe" in building something that would have all the same abilities as a human?" -> most probably you would need at minimum some pre-runtime checks (static type check, format proof verification, etc.) that the AGI/HLAI will be safe at runtime
- "What are the first goals/ deliveries for the startup?" - chup4cabra -> we will decide together what would be the best project to focus on working first so that we can obtain enough funds out of it to keep paying the employees and hire more staff
- "Who is doing the logo?" - FI5Ky -> that is something we will be brainstorming after I create the company
- "So essentially your goal is to get companies to communicate together and focus more on AI safety because of pascals wager?" - FI5Ky -> Pascal's Wager is non-sense so I would not base any of my arguments on it
- "You're saying we should care about ai safety because it could end in human extinction and we should.d prepare for worse case. or what are your arguments that we need to focus on AI safety?" - FI5Ky -> no, what I am saying is that if we do not engineer AGI to be safe (AGI safety is totally ignored by all companies doing AGI right now: OpenAI, DeepMind, etc.) then it will probably end up being unsafe since there are much more combinations of dangerous AGI than a safe one. And because of that we really need to focus on AGI/HLAI safety. Unlike with airbags in cars, we might not get a chance to fix our mistakes /after/ we created dangerous AGI/HLAI
- "What do you consider Intelligent?" - daintydikdik -> personally I think that all life forms exhibit some level of intelligence that is above 0. Intelligence is the capability of an agent to achieve non-trivial goals
- "What was the website with the pros and cons called again?" - FI5Ky -> https://www.kialo.com/should-general-ai-have-fundamental-rights-6295 kialo.com, it is a great way to settle debates with other people in a friendly, civilized and efficient manner
- "What is the difference between deep learning and ML?" - hussamadil -> ML, Machine Learning is any type of algorithm which is engineered and does learning. Deep Learning is a sub-set of ML and a subset of all types of Neural Network algorithms. Deep Learning algorithms are using Neural Networks that have many layers
- "Pretend I have a million dollars to invest, why should I invest it in your company?" - FI5Ky -> If your goals is to ensure a good future for all life forms then you share our goal and if you understand the plans we have you will understand why they will work and benefit everyone. Also, afaik. there is not anyone doing this at this level yet.